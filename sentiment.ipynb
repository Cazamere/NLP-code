{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cazamere/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/Cazamere/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 11081\n",
      "Train accuracy: 0.7768421052631579\n",
      "Test accuracy: 0.77\n",
      "unit -0.5841165547949962\n",
      "bad -0.7724425741764872\n",
      "cable 0.6503855545309271\n",
      "time -0.6832610317980353\n",
      "'ve 0.7581205146382877\n",
      "month -0.7114674028119194\n",
      "sound 0.9849168329889696\n",
      "lot 0.6401644468599978\n",
      "you 1.0055349112203857\n",
      "n't -2.1862891034776455\n",
      "easy 1.6918371704005468\n",
      "quality 1.343385077470172\n",
      "company -0.5808117691053474\n",
      "card -0.5344518084981137\n",
      "item -0.9622834903963183\n",
      "wa -1.5686920864089087\n",
      "perfect 1.0309853009658934\n",
      "fast 0.8965390260033751\n",
      "ha 0.5610378692670918\n",
      "price 2.7859008990834346\n",
      "money -1.0848265153448624\n",
      "memory 0.8645630219819426\n",
      "picture 0.5654551456705799\n",
      "buy -0.9083079648140405\n",
      "bit 0.5812747231720681\n",
      "pretty 0.7874716510164428\n",
      "doe -1.2733528093422333\n",
      "highly 0.9564517393284133\n",
      "recommend 0.6729810781679721\n",
      "fit 0.502657270620941\n",
      "customer -0.7099674518786546\n",
      "support -0.8634967813425914\n",
      "little 1.0002737662553502\n",
      "returned -0.7899976540348266\n",
      "excellent 1.2660860494983497\n",
      "love 1.1242988684849322\n",
      "home 0.5306694464233317\n",
      "week -0.7115526876329289\n",
      "using 0.612118984251188\n",
      "laptop 0.5065787201918209\n",
      "video 0.5287030974506407\n",
      "poor -0.7839827152224726\n",
      "then -0.9983084814250138\n",
      "tried -0.800684311972078\n",
      "try -0.6471866075334968\n",
      "comfortable 0.563362867209753\n",
      "hour -0.5497876106584504\n",
      "expected 0.5900476067484001\n",
      "speaker 0.8719399391222189\n",
      "warranty -0.6771232828564697\n",
      "stopped -0.5430167495970086\n",
      "junk -0.5466480497183679\n",
      "returning -0.5300965347339225\n",
      "paper 0.6221511824633096\n",
      "return -1.213043501784425\n",
      "waste -1.0109475593645236\n",
      "refund -0.6452800537649365\n",
      "Most wrong positive review (prob = 0.34946157745454054, pred = 0.0):\n",
      "\n",
      "A device like this either works or it doesn't.  This one happens to work\n",
      "\n",
      "Most wrong negative review (prob = 0.5988682075216419, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "# note: an alternative source of stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "\n",
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "\n",
    "# np.random.shuffle(positive_reviews)\n",
    "# positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n",
    "\n",
    "diff = len(positive_reviews) - len(negative_reviews)\n",
    "idxs = np.random.choice(len(negative_reviews), size=diff)\n",
    "extra = [negative_reviews[i] for i in idxs]\n",
    "negative_reviews += extra\n",
    "\n",
    "\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))\n",
    "\n",
    "\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "\n",
    "\n",
    "\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
    "\n",
    "\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
